{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Deep Q-Networks on Playing Games\n",
    "Reference Codes: https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "\n",
    "#### Data: \n",
    "Here the data is a game environment offered by OpenAI Gym. Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Tetris and chess. \n",
    "\n",
    "### Reinforcement Learning \n",
    "Reinforcement Learning is learning through interaction with an environment by taking a sequence of actions and by experiencing failures and successes while trying to maximize the rewards. Reinforcement learning systems have 4 main elements:\n",
    "-Policy (A mapping from the perceived states of the environment to the actions to be taken when in those states)\n",
    "-Reward signal (Given directly from the enviroment at each step)\n",
    "-Value function (The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting in that state)\n",
    "-Model\n",
    "\n",
    "#### Q-Learning: \n",
    "Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It’s considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn’t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward. \n",
    "\n",
    "#### DQN:\n",
    "If the combinations of states and actions are too large, the memory and the computation requirement for Q will be too high. To address that, switch to a deep network Q (DQN) to approximate Q(s, a). The learning algorithm is called Deep Q-learning. The new approach generalizes the approximation of the Q-value function rather than remembering the solutions. There are two neural networks here, the evaluate network and the target network. They have exactly the same structure but different parameters. The evaluate network is to retrive Q values while the target neural network is to store all updates in the training. After a certain steps of training, here replace_target_iter=300, the two networks would be synchronized. The purpose is to fix the Q-value targets temporarily so we don’t have a moving target to chase. With both experience replay and the target network, we have a more stable input and output to train the network and behaves more like supervised training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the DQN model\n",
    "Class DeepQNetWork is to build the deep q network for training the game. \n",
    "\n",
    "##### __init__: \n",
    "To initialize all  the parameters to be used in this model\n",
    ":param n_actions: Number of available actions to be taken in the game\n",
    ":param n_features: Number of features\n",
    ":param learning_rate: (.1)Gamma, how fast the model learns from the difference between actual and predict\n",
    ":param reward_decay: (.9)Decay rates on future reward\n",
    ":param e_greedy: (.9)(Greedy strategy). Maixmum epsilon:0.9, 90% of the time the model choose to maximize the value according to the table.\n",
    ":param replace_target_iter: Number of steps to update target_net\n",
    ":param memory_size: Maximum number of states-action pair that the model can remember\n",
    ":param batch_size: Number of states-action pair to take from memmory each time\n",
    ":param e_greedy_increment: Increment of epsilon\n",
    ":param output_graph: If true, output tensorboard file\n",
    "Memory: Q-Learning table\n",
    "learn_step_counter: Record current number of learning steps. When learn_step_counter reaches to replace_target_iter, the model will update target_net\n",
    "          \n",
    "##### _build_net: \n",
    "Build the two neural networks, the evaluate network and the target network. The evaluate network is to retrive Q values while the target neural network is to store all updates in the training. After a certain steps of training, here replace_target_iter=300, the two networks would be synchronized.\n",
    "\n",
    "##### store_transition:\n",
    "Off-Policy. Record all the steps that the model has gone through. On each call, it records one (state, action,  reward, next state) memory. When current memory reaches to the maximum memory size, replace the old memory with new memory. \n",
    "\n",
    "##### choose_action: \n",
    "This function decides which action to take at each step. 90% of the chance it would select a action that maximizes the value of action from the eval_net. 10% of the chance it would select a random action from all available actions. \n",
    "\n",
    "##### learn: \n",
    "How the model actually study form QDN (Update the q leanring value table). It would first check if the number of steps reaches to the maximum number of steps to update target_net and update accordingly. Then it randomly choose a batch of memory from available memories. Here, evaluate network and target network would synchronize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network off-policy\n",
    "class DeepQNetwork(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Number of available actions to be taken in the game\n",
    "            n_actions, \n",
    "            n_features, \n",
    "            # Gamma, how fast the model learns from the difference between actual and predict\n",
    "            learning_rate=0.01,\n",
    "            # Decay rates on future reward\n",
    "            reward_decay=0.9,\n",
    "            #(Greedy strategy). Maixmum epsilon:0.9, 90% of the time the model choose to maximize the value according to the table.\n",
    "            e_greedy=0.9,\n",
    "            # Number of steps to synchronize target_net\n",
    "            replace_target_iter=300,\n",
    "            # Maximum number of states-action pair that the model can remember\n",
    "            memory_size=500,\n",
    "            # Number of states-action pair to take from memmory each time\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions \n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        \n",
    "        \n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "        \n",
    "        # get parameters from target_net\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        \n",
    "        # get parameters from eval_net\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        \n",
    "        # update parameters of target_net\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # record the changes of cost\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "    \n",
    "    \n",
    "    # Records all the steps the model has gone through. \n",
    "    # On each call, it records one (state, action, reward, next state) memory. \n",
    "    # When current memory reaches to the maximum memory size, replace the old memory with new memory.\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    # How the model actually study form QDN. \n",
    "    # It would first check if the number of steps reaches to the maximum number of steps to update target_net and update accordingly. \n",
    "    # Then it randomly choose a batch of memory from available memories.\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        \n",
    "        # get q_next\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],  # fixed params\n",
    "                self.s: batch_memory[:, :self.n_features],  # newest params\n",
    "            })\n",
    "\n",
    "        # change q_target w.r.t q_eval's action\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        # train eval network\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "    def plot_cost(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DQN model to train the mountain car\n",
    "This part is to use the QDN model built above and train the car to go to the peak of the mountain. At each episode, the model observes the initial environment from last episode. At each step of the current episode, the model will choose an action to take given the current state. (In the earlier training stage, the greedy epsilon is small, most of the time the model would choose a random action rather than trying to maximize the value according to the table.) After choosing action at each step, the model would observe a state offered by the environment as a correspond to the action that the model has chosen. Then the model would store this (curr_state, action, reward, next_state) when the model accumulates certain amount of memories(steps), the model would start to learn(update the q learning table by bellman function) and then update the current state to next state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  0 | Get | Ep_r:  1893.9832 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  1 | Get | Ep_r:  390.3249 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  2 | Get | Ep_r:  159.4569 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  3 | Get | Ep_r:  85.7349 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  4 | Get | Ep_r:  142.45 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  5 | Get | Ep_r:  125.1597 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  6 | Get | Ep_r:  177.7879 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  7 | Get | Ep_r:  94.4323 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  8 | Get | Ep_r:  128.5074 | Epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "Epi:  9 | Get | Ep_r:  107.4875 | Epsilon:  0.9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "RL = DeepQNetwork(n_actions=3, n_features=2, learning_rate=0.001, e_greedy=0.9,\n",
    "                  replace_target_iter=300, memory_size=3000,\n",
    "                  e_greedy_increment=0.0002,)\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "for i_episode in range(10):\n",
    "\n",
    "    observation = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        # Refresh the environment \n",
    "        env.render()\n",
    "        \n",
    "        # DQN chooses action according to the observation \n",
    "        action = RL.choose_action(observation)\n",
    "        \n",
    "        # The environment gives the next state, reward and check if terminate\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "\n",
    "        position, velocity = observation_\n",
    "\n",
    "        # the higher the better\n",
    "        reward = abs(position - (-0.5))     # r in [0, 1]\n",
    "        \n",
    "        # DQN stores memories \n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        if total_steps > 1000:\n",
    "            RL.learn()\n",
    "\n",
    "        ep_r += reward\n",
    "        if done:\n",
    "            get = '| Get' if observation_[0] >= env.unwrapped.goal_position else '| ----'\n",
    "            print('Epi: ', i_episode,\n",
    "                  get,\n",
    "                  '| Ep_r: ', round(ep_r, 4),\n",
    "                  '| Epsilon: ', round(RL.epsilon, 2))\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cost plot\n",
    "Unlike supervised learning, the cost function plot is not smoothly declined. This is because the input of DQN is changing after learning from each step. (The 'label'is not consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
